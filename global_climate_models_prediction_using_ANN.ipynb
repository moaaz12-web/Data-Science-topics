{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZl22r8EiArw"
      },
      "source": [
        "# IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAuPUHgopy-M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcLCg1KCfKyc",
        "outputId": "ba4deb65-cb99-474c-f0aa-573b56fc7565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6JiElbIbYcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLSK2YFJhOw-"
      },
      "source": [
        "# PROCESSING AND READING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzWgbNsDqblr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Provide the path to your Excel file\n",
        "excel_file_path = '/content/drive/MyDrive/GCM Historic data for downscaling .xlsx'\n",
        "\n",
        "# Read all sheets into a dictionary of DataFrames\n",
        "sheets_dict = pd.read_excel(excel_file_path, sheet_name=None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gcm_10_path = '/content/drive/MyDrive/GCM 10-NORESM2-LM.xlsx'\n",
        "df = pd.read_excel(gcm_10_path, sheet_name=None)\n"
      ],
      "metadata": {
        "id": "rtDd4qq_VgPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['GCM 10..'].columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyueFwYOW2L0",
        "outputId": "e42b3732-bcd9-4131-c82e-328b452b557f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['DATE', 'PPT FOR DWONSCALLING', 'TAS MAX FOR DWONSCALLING',\n",
              "       'TAS MIN FOR DWONSCALLING', 'PPT FOR DWONSCALLING.1',\n",
              "       'TAS MAX FOR DWONSCALLING.1', 'TAS MIN FOR DWONSCALLING.1',\n",
              "       'PPT FOR DWONSCALLING.2', 'TAS MAX FOR DWONSCALLING.2',\n",
              "       'TAS MIN FOR DWONSCALLING.2', 'PPT FOR DWONSCALLING.3',\n",
              "       'TAS MAX FOR DWONSCALLING.3', 'TAS MIN FOR DWONSCALLING.3',\n",
              "       'PPT FOR DWONSCALLING.4', 'TAS MAX FOR DWONSCALLING.4',\n",
              "       'TAS MIN FOR DWONSCALLING.4', 'PPT FOR DWONSCALLING.5',\n",
              "       'TAS MAX FOR DWONSCALLING.5', 'TAS MIN FOR DWONSCALLING.5',\n",
              "       'PPT FOR DWONSCALLING.6', 'TAS MAX FOR DWONSCALLING.6',\n",
              "       'TAS MIN FOR DWONSCALLING.6', 'PPT FOR DWONSCALLING.7',\n",
              "       'TAS MAX FOR DWONSCALLING.7', 'TAS MIN FOR DWONSCALLING.7',\n",
              "       'PPT FOR DWONSCALLING.8', 'TAS MAX FOR DWONSCALLING.8',\n",
              "       'TAS MIN FOR DWONSCALLING.8', 'PPT FOR DWONSCALLING.9',\n",
              "       'TAS MAX FOR DWONSCALLING.9', 'TAS MIN FOR DWONSCALLING.9',\n",
              "       'PPT FOR DWONSCALLING.10', 'TAS MAX FOR DWONSCALLING.10',\n",
              "       'TAS MIN FOR DWONSCALLING.10', 'PPT FOR DWONSCALLING.11',\n",
              "       'TAS MAX FOR DWONSCALLING.11', 'TAS MIN FOR DWONSCALLING.11'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GCM10_ppt = df['GCM 10..'].iloc[:11000, [1,4,7,10,13,16,19,22,25,28,31, 34]]\n",
        "GCM10_tass_max = df['GCM 10..'].iloc[:11000, [2,5,8,11,14,17,20,23,26,29,32, 35]]\n",
        "GCM10_tass_min = df['GCM 10..'].iloc[:11000, [3,6,9,12,15,18,21,24,27,30,33, 36]]"
      ],
      "metadata": {
        "id": "sO6_WmkyWFuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9CE_PKAWIVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uly8srISwQZo"
      },
      "outputs": [],
      "source": [
        "# Create an empty dictionary to store modified DataFrames\n",
        "modified_sheets_dict = {}\n",
        "\n",
        "# Iterate through each sheet and process the data\n",
        "for sheet_name, sheet_data in sheets_dict.items():\n",
        "    # Skip the first two rows and load the data into a DataFrame\n",
        "    modified_sheet_data = pd.read_excel(excel_file_path, sheet_name, skiprows=2)\n",
        "    modified_sheets_dict[sheet_name] = modified_sheet_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modified_sheets_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dqCaR-vW_Nk",
        "outputId": "57572024-311d-4c78-9f98-f698bd5031c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['OBSERVED DATA ', 'GCM  10 N0RESM2-LM', 'GCM  9 IPSL-CM6A-LR', 'GCM  8 MRI-ESM2', 'GCM 7 INM-CM5', 'GCM  6 INM-CM4', 'GCM 5 CMCC-ESM2', 'GCM 4  CAN-ESM5', 'GCM 3 CNRM-ESM2 ', 'GCM 2 CNRM-CM6', 'GCM 1 MIROC6'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bPrtdVTUkK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcB2Ebr42de"
      },
      "outputs": [],
      "source": [
        "for i in modified_sheets_dict.keys():\n",
        "  for j in modified_sheets_dict[i].columns:\n",
        "    if j == 'DATE' or j == 'Date' or j == 'DATE' or j == 'date':\n",
        "      # print(j)\n",
        "      modified_sheets_dict[i].drop([j], axis=1, inplace=True)\n",
        "\n",
        "for i in modified_sheets_dict.keys():\n",
        "  for j in modified_sheets_dict[i].columns:\n",
        "    if j == 'DATE' or j == 'Date' or j == 'DATE' or j=='date':\n",
        "      print(j)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns_ppt = list(range(12, 48, 3))\n",
        "selected_columns_tass_max = list(range(13, 48, 3))\n",
        "selected_columns_tass_min = list(range(14, 49, 3))\n",
        "\n",
        "# oBSERVED DATA\n",
        "Observed_ppt = modified_sheets_dict['OBSERVED DATA '].iloc[:11000, selected_columns_ppt]\n",
        "Observed_tass_max = modified_sheets_dict['OBSERVED DATA '].iloc[:11000, selected_columns_tass_max]\n",
        "Observed_tass_min = modified_sheets_dict['OBSERVED DATA '].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 1 MIROC6\n",
        "GCM1_ppt = modified_sheets_dict['GCM 1 MIROC6'].iloc[:11000, selected_columns_ppt]\n",
        "GCM1_tass_max = modified_sheets_dict['GCM 1 MIROC6'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM1_tass_min = modified_sheets_dict['GCM 1 MIROC6'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 2 CNRM-CM6\n",
        "GCM2_ppt = modified_sheets_dict['GCM 2 CNRM-CM6'].iloc[:11000, selected_columns_ppt]\n",
        "GCM2_tass_max = modified_sheets_dict['GCM 2 CNRM-CM6'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM2_tass_min = modified_sheets_dict['GCM 2 CNRM-CM6'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 3 CNRM-ESM2\n",
        "GCM3_ppt = modified_sheets_dict['GCM 3 CNRM-ESM2 '].iloc[:11000, selected_columns_ppt]\n",
        "GCM3_tass_max = modified_sheets_dict['GCM 3 CNRM-ESM2 '].iloc[:11000, selected_columns_tass_max]\n",
        "GCM3_tass_min = modified_sheets_dict['GCM 3 CNRM-ESM2 '].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 4 CAN-ESM5\n",
        "GCM4_ppt = modified_sheets_dict['GCM 4  CAN-ESM5'].iloc[:11000, selected_columns_ppt]\n",
        "GCM4_tass_max = modified_sheets_dict['GCM 4  CAN-ESM5'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM4_tass_min = modified_sheets_dict['GCM 4  CAN-ESM5'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 5 CMCC-ESM2\n",
        "GCM5_ppt = modified_sheets_dict['GCM 5 CMCC-ESM2'].iloc[:11000, selected_columns_ppt]\n",
        "GCM5_tass_max = modified_sheets_dict['GCM 5 CMCC-ESM2'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM5_tass_min = modified_sheets_dict['GCM 5 CMCC-ESM2'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 6 INM-CM4\n",
        "GCM6_ppt = modified_sheets_dict['GCM  6 INM-CM4'].iloc[:11000, selected_columns_ppt]\n",
        "GCM6_tass_max = modified_sheets_dict['GCM  6 INM-CM4'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM6_tass_min = modified_sheets_dict['GCM  6 INM-CM4'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 7 INM-CM5\n",
        "GCM7_ppt = modified_sheets_dict['GCM 7 INM-CM5'].iloc[:11000, selected_columns_ppt]\n",
        "GCM7_tass_max = modified_sheets_dict['GCM 7 INM-CM5'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM7_tass_min = modified_sheets_dict['GCM 7 INM-CM5'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 8 MRI-ESM2\n",
        "GCM8_ppt = modified_sheets_dict['GCM  8 MRI-ESM2'].iloc[:11000, selected_columns_ppt]\n",
        "GCM8_tass_max = modified_sheets_dict['GCM  8 MRI-ESM2'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM8_tass_min = modified_sheets_dict['GCM  8 MRI-ESM2'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 9 IPSL-CM6A-LR\n",
        "GCM9_ppt = modified_sheets_dict['GCM  9 IPSL-CM6A-LR'].iloc[:11000, selected_columns_ppt]\n",
        "GCM9_tass_max = modified_sheets_dict['GCM  9 IPSL-CM6A-LR'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM9_tass_min = modified_sheets_dict['GCM  9 IPSL-CM6A-LR'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 10 N0RESM2-LM\n",
        "GCM10_ppt = df['GCM 10..'].iloc[:11000, [1,4,7,10,13,16,19,22,25,28,31, 34]]\n",
        "GCM10_tass_max = df['GCM 10..'].iloc[:11000, [2,5,8,11,14,17,20,23,26,29,32, 35]]\n",
        "GCM10_tass_min = df['GCM 10..'].iloc[:11000, [3,6,9,12,15,18,21,24,27,30,33, 36]]\n"
      ],
      "metadata": {
        "id": "WjVjR_S8UMQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O8CqaPPt4tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0269be-77e5-4f27-b011-9f9df43d2269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-99598f7ef5d7>:28: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df.fillna(df.mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames with null values after filling:\n"
          ]
        }
      ],
      "source": [
        "# List of all GCM DataFrames\n",
        "gcm_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min,\n",
        "                        GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min\n",
        "                        ]\n",
        "# Loop through each DataFrame and fill missing values with mean for each column\n",
        "for df_name, df in zip(['Observed_ppt', 'Observed_tass_max', 'Observed_tass_min',\n",
        "                        'GCM1_ppt', 'GCM1_tass_max', 'GCM1_tass_min',\n",
        "                        'GCM2_ppt', 'GCM2_tass_max', 'GCM2_tass_min',\n",
        "                        'GCM3_ppt', 'GCM3_tass_max', 'GCM3_tass_min',\n",
        "                        'GCM4_ppt', 'GCM4_tass_max', 'GCM4_tass_min',\n",
        "                        'GCM5_ppt', 'GCM5_tass_max', 'GCM5_tass_min',\n",
        "                        'GCM6_ppt', 'GCM6_tass_max', 'GCM6_tass_min',\n",
        "                        'GCM7_ppt', 'GCM7_tass_max', 'GCM7_tass_min',\n",
        "                        'GCM8_ppt', 'GCM8_tass_max', 'GCM8_tass_min',\n",
        "                        'GCM9_ppt', 'GCM9_tass_max', 'GCM9_tass_min',\n",
        "                        'GCM10_ppt', 'GCM10_tass_max', 'GCM10_tass_min'\n",
        "                        ],\n",
        "                       gcm_data_frames_list):\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Loop through each DataFrame and check for null values after filling\n",
        "data_frames_with_nulls = []\n",
        "for df_name, df in zip(['Observed_ppt', 'Observed_tass_max', 'Observed_tass_min',\n",
        "                        'GCM1_ppt', 'GCM1_tass_max', 'GCM1_tass_min',\n",
        "                        'GCM2_ppt', 'GCM2_tass_max', 'GCM2_tass_min',\n",
        "                        'GCM3_ppt', 'GCM3_tass_max', 'GCM3_tass_min',\n",
        "                        'GCM4_ppt', 'GCM4_tass_max', 'GCM4_tass_min',\n",
        "                        'GCM5_ppt', 'GCM5_tass_max', 'GCM5_tass_min',\n",
        "                        'GCM6_ppt', 'GCM6_tass_max', 'GCM6_tass_min',\n",
        "                        'GCM7_ppt', 'GCM7_tass_max', 'GCM7_tass_min',\n",
        "                        'GCM8_ppt', 'GCM8_tass_max', 'GCM8_tass_min',\n",
        "                        'GCM9_ppt', 'GCM9_tass_max', 'GCM9_tass_min',\n",
        "                        'GCM10_ppt', 'GCM10_tass_max', 'GCM10_tass_min'\n",
        "                        ],\n",
        "                       gcm_data_frames_list):\n",
        "    if df.isnull().values.any():\n",
        "        data_frames_with_nulls.append(df_name)\n",
        "\n",
        "# Display DataFrame names with null values after filling\n",
        "print(\"DataFrames with null values after filling:\")\n",
        "for df_name in data_frames_with_nulls:\n",
        "    print(df_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all GCM DataFrames\n",
        "gcm_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min,\n",
        "                        GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min\n",
        "                        ]\n",
        "\n",
        "# Loop through each DataFrame and check for null values\n",
        "for df_name, df in zip(['Observed_ppt', 'Observed_tass_max', 'Observed_tass_min',\n",
        "                        'GCM1_ppt', 'GCM1_tass_max', 'GCM1_tass_min',\n",
        "                        'GCM2_ppt', 'GCM2_tass_max', 'GCM2_tass_min',\n",
        "                        'GCM3_ppt', 'GCM3_tass_max', 'GCM3_tass_min',\n",
        "                        'GCM4_ppt', 'GCM4_tass_max', 'GCM4_tass_min',\n",
        "                        'GCM5_ppt', 'GCM5_tass_max', 'GCM5_tass_min',\n",
        "                        'GCM6_ppt', 'GCM6_tass_max', 'GCM6_tass_min',\n",
        "                        'GCM7_ppt', 'GCM7_tass_max', 'GCM7_tass_min',\n",
        "                        'GCM8_ppt', 'GCM8_tass_max', 'GCM8_tass_min',\n",
        "                        'GCM9_ppt', 'GCM9_tass_max', 'GCM9_tass_min',\n",
        "                        'GCM10_ppt', 'GCM10_tass_max', 'GCM10_tass_min'\n",
        "                        ],\n",
        "                       gcm_data_frames_list):\n",
        "    if df.isnull().values.any():\n",
        "        print(f\"DataFrame {df_name} contains null values.\")\n"
      ],
      "metadata": {
        "id": "BtlhG5i1bCwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4npOhYgybLB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "lYUbd4Ecb_t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcm_data_frames_list = [GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min,\n",
        "                        ]\n",
        "\n",
        "observed_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min]\n"
      ],
      "metadata": {
        "id": "DFaWYMA9bK81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# def calculate_nse(y_true, y_pred):\n",
        "#     nse = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "#     return nse\n",
        "\n",
        "# def calculate_pbias(y_true, y_pred):\n",
        "#     pbias = np.sum(y_pred - y_true) / np.sum(y_true) * 100\n",
        "#     return pbias\n",
        "\n",
        "# gcm_data_frames_list = [GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "#                         GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "#                         GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "#                         GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "#                         GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "#                         GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "#                         GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "#                         GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "#                         GCM9_ppt, GCM9_tass_max, GCM9_tass_min]\n",
        "\n",
        "# observed_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min]\n",
        "\n",
        "# for i, (gcm_df, observed_df) in enumerate(zip(gcm_data_frames_list, observed_data_frames_list)):\n",
        "#     print(f\"Training ANN for GCM {i+1}...\")\n",
        "\n",
        "#     # Handle non-numeric values in target_data\n",
        "#     # observed_df[observed_df == 'R'] = 2.3\n",
        "#     # observed_df[observed_df == '----'] = 4.2\n",
        "#     # observed_df = observed_df.astype(float)\n",
        "\n",
        "#     # Handle non-numeric values in target_data\n",
        "#     observed_df = observed_df.applymap(lambda x: 2.3 if x == 'R' else (4.2 if x == '----' else x))\n",
        "#     observed_df = observed_df.astype(float)\n",
        "#     gcm_df = gcm_df.applymap(lambda x: 2.3 if x == 'R' else (4.2 if x == '----' else x))\n",
        "#     gcm_df = gcm_df.astype(float)\n",
        "\n",
        "\n",
        "\n",
        "#     # Split the data into features (X) and target (y)\n",
        "#     X = gcm_df.values\n",
        "#     y = observed_df.values\n",
        "\n",
        "\n",
        "\n",
        "#     # Split the data into training and testing sets\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#     x_train = X_train.astype(np.float)\n",
        "#     y_train = y_train.astype(np.float)\n",
        "#     x_test = X_train.astype(np.float)\n",
        "#     y_test = y_test.astype(np.float)\n",
        "\n",
        "#     # Standardize the features\n",
        "#     scaler = StandardScaler()\n",
        "#     X_train_scaled = scaler.fit_transform(X_train)\n",
        "#     X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#     # Build the neural network architecture\n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "#         tf.keras.layers.Dense(50, activation='relu'),\n",
        "#         tf.keras.layers.Dense(50, activation='relu'),\n",
        "#         tf.keras.layers.Dense(y_train.shape[1])  # Output layer\n",
        "#     ])\n",
        "\n",
        "#     # Compile the model\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(X_train_scaled, y_train, epochs=1, batch_size=128, verbose=0)\n",
        "\n",
        "#     # Make predictions\n",
        "#     y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "#     # Calculate R-squared, NSE, and PBIAS\n",
        "#     r2 = r2_score(y_test, y_pred)\n",
        "#     nse = calculate_nse(y_test, y_pred)\n",
        "#     pbias = calculate_pbias(y_test, y_pred)\n",
        "\n",
        "#     print(f\"R-squared for GCM {i+1}: {r2}\")\n",
        "#     print(f\"NSE for GCM {i+1}: {nse}\")\n",
        "#     print(f\"PBIAS for GCM {i+1}: {pbias}\")"
      ],
      "metadata": {
        "id": "MiK7T0w1bK4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def calculate_nse(y_true, y_pred):\n",
        "    nse = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "    return nse\n",
        "\n",
        "def calculate_pbias(y_true, y_pred):\n",
        "    pbias = np.sum(y_pred - y_true) / np.sum(y_true) * 100\n",
        "    return pbias\n",
        "\n",
        "gcm_data_list = [GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min,\n",
        "]\n",
        "\n",
        "# Observed dataframes\n",
        "observed_data_list = [Observed_ppt, Observed_tass_max, Observed_tass_min]\n",
        "\n",
        "# Lists to store results\n",
        "r2_results = []\n",
        "nse_results = []\n",
        "pbias_results = []\n",
        "\n",
        "# Create directories for saving files\n",
        "if not os.path.exists('model_files'):\n",
        "    os.makedirs('model_files')\n",
        "\n",
        "if not os.path.exists('testing_predictions'):\n",
        "    os.makedirs('testing_predictions')\n",
        "\n",
        "if not os.path.exists('actual_testing_data'):\n",
        "    os.makedirs('actual_testing_data')\n",
        "\n",
        "if not os.path.exists('actual_testing_data_Xs'):\n",
        "    os.makedirs('actual_testing_data_Xs')\n",
        "\n",
        "\n",
        "# Loop through each GCM\n",
        "for i in range(1, 11):\n",
        "    gcm_start_idx = (i-1)*3\n",
        "    gcm_df_list = gcm_data_list[gcm_start_idx:gcm_start_idx+3]\n",
        "    # Concatenate GCM data\n",
        "    gcm_df = pd.concat(gcm_df_list, axis=1)\n",
        "\n",
        "    # Concatenate observed data\n",
        "    observed_df = pd.concat(observed_data_list, axis=1)\n",
        "\n",
        "    print(f\"Training model for GCM{i}\")\n",
        "\n",
        "    # Preprocess data\n",
        "    gcm_df = gcm_df.applymap(lambda x: 2.3 if x=='R' else (4.2 if x=='----' else x))\n",
        "    gcm_df = gcm_df.astype(float)\n",
        "\n",
        "    observed_df = observed_df.applymap(lambda x: 2.3 if x=='R' else (4.2 if x=='----' else x))\n",
        "    observed_df = observed_df.astype(float)\n",
        "\n",
        "    # Split data\n",
        "    X = gcm_df.values\n",
        "    y = observed_df.values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Build and train model\n",
        "    model = tf.keras.Sequential([\n",
        "       tf.keras.layers.Dense(128, activation='relu'),\n",
        "       tf.keras.layers.Dropout(0.4),\n",
        "       tf.keras.layers.Dense(50, activation='relu'),\n",
        "       tf.keras.layers.Dense(y_train.shape[1])\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train_scaled, y_train, epochs=30, batch_size=256, verbose=0)\n",
        "\n",
        "    # Evaluate model\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    nse = calculate_nse(y_test, y_pred)\n",
        "    pbias = calculate_pbias(y_test, y_pred)\n",
        "\n",
        "    print(f\"GCM {i} Results:\")\n",
        "    print(f\"R-squared: {r2}\")\n",
        "    print(f\"NSE: {nse}\")\n",
        "    print(f\"PBIAS: {pbias}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Save model\n",
        "    model.save(os.path.join('model_files', f'gcm_model_{i}.keras'))\n",
        "\n",
        "    # Save results\n",
        "    r2_results.append(r2)\n",
        "    nse_results.append(nse)\n",
        "    pbias_results.append(pbias)\n",
        "\n",
        "    # Save testing predictions and actual testing data to CSV\n",
        "    testing_predictions_df = pd.DataFrame(data=y_pred, columns=observed_df.columns)\n",
        "    actual_testing_data_df = pd.DataFrame(data=y_test, columns=observed_df.columns)\n",
        "    actual_testing_data_Xs = pd.DataFrame(data=X_test, columns=observed_df.columns)\n",
        "\n",
        "    testing_predictions_df.to_csv(os.path.join('testing_predictions', f'testing_predictions_gcm{i}.csv'), index=False)\n",
        "    actual_testing_data_df.to_csv(os.path.join('actual_testing_data', f'actual_testing_data_gcm{i}.csv'), index=False)\n",
        "    actual_testing_data_Xs.to_csv(os.path.join('actual_testing_data_Xs', f'actual_testing_data_Xs_gcm{i}.csv'), index=False)\n",
        "\n",
        "# Create a summary dataframe for results\n",
        "results_df = pd.DataFrame({\n",
        "    'GCM': range(1, 11),\n",
        "    'R-squared': r2_results,\n",
        "    'NSE': nse_results,\n",
        "    'PBIAS': pbias_results\n",
        "})\n",
        "\n",
        "# Save results summary to CSV\n",
        "results_df.to_csv('results_summary.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJNr4PX4pOe2",
        "outputId": "edf7bebe-40ac-4ec1-80c5-80381fc0ec01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for GCM1\n",
            "104/104 [==============================] - 0s 1ms/step\n",
            "GCM 1 Results:\n",
            "R-squared: 0.5367989813325258\n",
            "NSE: 0.8351535511021142\n",
            "PBIAS: -2.2115415368929083\n",
            "\n",
            "\n",
            "Training model for GCM2\n",
            "104/104 [==============================] - 0s 1ms/step\n",
            "GCM 2 Results:\n",
            "R-squared: 0.5259631625001912\n",
            "NSE: 0.8293147997505284\n",
            "PBIAS: -1.6925881277526633\n",
            "\n",
            "\n",
            "Training model for GCM3\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 3 Results:\n",
            "R-squared: 0.52245308353175\n",
            "NSE: 0.8270622921360977\n",
            "PBIAS: -1.664770643206947\n",
            "\n",
            "\n",
            "Training model for GCM4\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 4 Results:\n",
            "R-squared: 0.5356125296775861\n",
            "NSE: 0.8344607933731444\n",
            "PBIAS: -2.9887997703321005\n",
            "\n",
            "\n",
            "Training model for GCM5\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 5 Results:\n",
            "R-squared: 0.4928802480647051\n",
            "NSE: 0.8107856789382194\n",
            "PBIAS: -1.8830819903868468\n",
            "\n",
            "\n",
            "Training model for GCM6\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 6 Results:\n",
            "R-squared: 0.44417598240359335\n",
            "NSE: 0.7845196844236598\n",
            "PBIAS: -0.07371500360745432\n",
            "\n",
            "\n",
            "Training model for GCM7\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 7 Results:\n",
            "R-squared: 0.4585090092038263\n",
            "NSE: 0.792674490613269\n",
            "PBIAS: -2.7665886084347897\n",
            "\n",
            "\n",
            "Training model for GCM8\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 8 Results:\n",
            "R-squared: 0.5169120527563151\n",
            "NSE: 0.8245431756365175\n",
            "PBIAS: -2.079582713937304\n",
            "\n",
            "\n",
            "Training model for GCM9\n",
            "104/104 [==============================] - 0s 1ms/step\n",
            "GCM 9 Results:\n",
            "R-squared: 0.517468262457603\n",
            "NSE: 0.8250495724434581\n",
            "PBIAS: -0.704371351754603\n",
            "\n",
            "\n",
            "Training model for GCM10\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 10 Results:\n",
            "R-squared: 0.2691550807203884\n",
            "NSE: 0.6938630745745282\n",
            "PBIAS: -1.557625182344221\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/testing_predictions'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'testing_predictions.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "id": "w--q2xJvqLkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8944f70d-edda-41a1-c60f-e68ec28d4760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'testing_predictions.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/actual_testing_data_Xs'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'actual_testing_data_Xs.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDGmjQRFqLgf",
        "outputId": "f6177424-1a5b-40b1-9aba-b4993719c9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'actual_testing_data_Xs.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/actual_testing_data'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'actual_testing_data.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNRlXUMP67wa",
        "outputId": "d861d8b0-ab95-42b7-965c-107457cb3e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'actual_testing_data.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5GZ6r3d62M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/model_files'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'model_files.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLY9oq4_rNIr",
        "outputId": "7cdf5ca5-657f-4a2e-840c-e4980d96abd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'model_files.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/ANN_results_1-10'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'ANN_results_1-10.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "id": "GPnuDuSWrSOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa206f36-e906-45b2-a94f-a6f72353fa28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'ANN_results_1-10.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8o1lWKtakAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
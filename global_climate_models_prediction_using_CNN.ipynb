{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZl22r8EiArw"
      },
      "source": [
        "# IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAuPUHgopy-M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcLCg1KCfKyc",
        "outputId": "6f619974-f7f2-414d-b27c-ba139c495287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLSK2YFJhOw-"
      },
      "source": [
        "# PROCESSING AND READING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzWgbNsDqblr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Provide the path to your Excel file\n",
        "excel_file_path = '/content/drive/MyDrive/GCM Historic data for downscaling .xlsx'\n",
        "\n",
        "# Read all sheets into a dictionary of DataFrames\n",
        "sheets_dict = pd.read_excel(excel_file_path, sheet_name=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gcm_10_path = '/content/drive/MyDrive/GCM 10-NORESM2-LM.xlsx'\n",
        "df = pd.read_excel(gcm_10_path, sheet_name=None)"
      ],
      "metadata": {
        "id": "rtDd4qq_VgPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyueFwYOW2L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GCM10_ppt = df['GCM 10..'].iloc[:11000, [1,4,7,10,13,16,19,22,25,28,31, 34]]\n",
        "GCM10_tass_max = df['GCM 10..'].iloc[:11000, [2,5,8,11,14,17,20,23,26,29,32, 35]]\n",
        "GCM10_tass_min = df['GCM 10..'].iloc[:11000, [3,6,9,12,15,18,21,24,27,30,33, 36]]"
      ],
      "metadata": {
        "id": "sO6_WmkyWFuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uly8srISwQZo"
      },
      "outputs": [],
      "source": [
        "# Create an empty dictionary to store modified DataFrames\n",
        "modified_sheets_dict = {}\n",
        "\n",
        "# Iterate through each sheet and process the data\n",
        "for sheet_name, sheet_data in sheets_dict.items():\n",
        "    # Skip the first two rows and load the data into a DataFrame\n",
        "    modified_sheet_data = pd.read_excel(excel_file_path, sheet_name, skiprows=2)\n",
        "    modified_sheets_dict[sheet_name] = modified_sheet_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modified_sheets_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dqCaR-vW_Nk",
        "outputId": "da4141ce-ea53-4885-98db-159f89c5a6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['OBSERVED DATA ', 'GCM  10 N0RESM2-LM', 'GCM  9 IPSL-CM6A-LR', 'GCM  8 MRI-ESM2', 'GCM 7 INM-CM5', 'GCM  6 INM-CM4', 'GCM 5 CMCC-ESM2', 'GCM 4  CAN-ESM5', 'GCM 3 CNRM-ESM2 ', 'GCM 2 CNRM-CM6', 'GCM 1 MIROC6'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bPrtdVTUkK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmcB2Ebr42de"
      },
      "outputs": [],
      "source": [
        "for i in modified_sheets_dict.keys():\n",
        "  for j in modified_sheets_dict[i].columns:\n",
        "    if j == 'DATE' or j == 'Date' or j == 'DATE' or j == 'date':\n",
        "      # print(j)\n",
        "      modified_sheets_dict[i].drop([j], axis=1, inplace=True)\n",
        "\n",
        "for i in modified_sheets_dict.keys():\n",
        "  for j in modified_sheets_dict[i].columns:\n",
        "    if j == 'DATE' or j == 'Date' or j == 'DATE' or j=='date':\n",
        "      print(j)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns_ppt = list(range(12, 48, 3))\n",
        "selected_columns_tass_max = list(range(13, 48, 3))\n",
        "selected_columns_tass_min = list(range(14, 49, 3))\n",
        "\n",
        "# oBSERVED DATA\n",
        "Observed_ppt = modified_sheets_dict['OBSERVED DATA '].iloc[:11000, selected_columns_ppt]\n",
        "Observed_tass_max = modified_sheets_dict['OBSERVED DATA '].iloc[:11000, selected_columns_tass_max]\n",
        "Observed_tass_min = modified_sheets_dict['OBSERVED DATA '].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 1 MIROC6\n",
        "GCM1_ppt = modified_sheets_dict['GCM 1 MIROC6'].iloc[:11000, selected_columns_ppt]\n",
        "GCM1_tass_max = modified_sheets_dict['GCM 1 MIROC6'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM1_tass_min = modified_sheets_dict['GCM 1 MIROC6'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 2 CNRM-CM6\n",
        "GCM2_ppt = modified_sheets_dict['GCM 2 CNRM-CM6'].iloc[:11000, selected_columns_ppt]\n",
        "GCM2_tass_max = modified_sheets_dict['GCM 2 CNRM-CM6'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM2_tass_min = modified_sheets_dict['GCM 2 CNRM-CM6'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 3 CNRM-ESM2\n",
        "GCM3_ppt = modified_sheets_dict['GCM 3 CNRM-ESM2 '].iloc[:11000, selected_columns_ppt]\n",
        "GCM3_tass_max = modified_sheets_dict['GCM 3 CNRM-ESM2 '].iloc[:11000, selected_columns_tass_max]\n",
        "GCM3_tass_min = modified_sheets_dict['GCM 3 CNRM-ESM2 '].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 4 CAN-ESM5\n",
        "GCM4_ppt = modified_sheets_dict['GCM 4  CAN-ESM5'].iloc[:11000, selected_columns_ppt]\n",
        "GCM4_tass_max = modified_sheets_dict['GCM 4  CAN-ESM5'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM4_tass_min = modified_sheets_dict['GCM 4  CAN-ESM5'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 5 CMCC-ESM2\n",
        "GCM5_ppt = modified_sheets_dict['GCM 5 CMCC-ESM2'].iloc[:11000, selected_columns_ppt]\n",
        "GCM5_tass_max = modified_sheets_dict['GCM 5 CMCC-ESM2'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM5_tass_min = modified_sheets_dict['GCM 5 CMCC-ESM2'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 6 INM-CM4\n",
        "GCM6_ppt = modified_sheets_dict['GCM  6 INM-CM4'].iloc[:11000, selected_columns_ppt]\n",
        "GCM6_tass_max = modified_sheets_dict['GCM  6 INM-CM4'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM6_tass_min = modified_sheets_dict['GCM  6 INM-CM4'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 7 INM-CM5\n",
        "GCM7_ppt = modified_sheets_dict['GCM 7 INM-CM5'].iloc[:11000, selected_columns_ppt]\n",
        "GCM7_tass_max = modified_sheets_dict['GCM 7 INM-CM5'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM7_tass_min = modified_sheets_dict['GCM 7 INM-CM5'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 8 MRI-ESM2\n",
        "GCM8_ppt = modified_sheets_dict['GCM  8 MRI-ESM2'].iloc[:11000, selected_columns_ppt]\n",
        "GCM8_tass_max = modified_sheets_dict['GCM  8 MRI-ESM2'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM8_tass_min = modified_sheets_dict['GCM  8 MRI-ESM2'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 9 IPSL-CM6A-LR\n",
        "GCM9_ppt = modified_sheets_dict['GCM  9 IPSL-CM6A-LR'].iloc[:11000, selected_columns_ppt]\n",
        "GCM9_tass_max = modified_sheets_dict['GCM  9 IPSL-CM6A-LR'].iloc[:11000, selected_columns_tass_max]\n",
        "GCM9_tass_min = modified_sheets_dict['GCM  9 IPSL-CM6A-LR'].iloc[:11000, selected_columns_tass_min]\n",
        "\n",
        "# GCM 10 N0RESM2-LM\n",
        "GCM10_ppt = df['GCM 10..'].iloc[:11000, [1,4,7,10,13,16,19,22,25,28,31, 34]]\n",
        "GCM10_tass_max = df['GCM 10..'].iloc[:11000, [2,5,8,11,14,17,20,23,26,29,32, 35]]\n",
        "GCM10_tass_min = df['GCM 10..'].iloc[:11000, [3,6,9,12,15,18,21,24,27,30,33, 36]]\n"
      ],
      "metadata": {
        "id": "WjVjR_S8UMQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O8CqaPPt4tx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dfdfb92-5ff3-4b90-b081-1b59815c74be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-99598f7ef5d7>:28: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  df.fillna(df.mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames with null values after filling:\n"
          ]
        }
      ],
      "source": [
        "# List of all GCM DataFrames\n",
        "gcm_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min,\n",
        "                        GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min\n",
        "                        ]\n",
        "# Loop through each DataFrame and fill missing values with mean for each column\n",
        "for df_name, df in zip(['Observed_ppt', 'Observed_tass_max', 'Observed_tass_min',\n",
        "                        'GCM1_ppt', 'GCM1_tass_max', 'GCM1_tass_min',\n",
        "                        'GCM2_ppt', 'GCM2_tass_max', 'GCM2_tass_min',\n",
        "                        'GCM3_ppt', 'GCM3_tass_max', 'GCM3_tass_min',\n",
        "                        'GCM4_ppt', 'GCM4_tass_max', 'GCM4_tass_min',\n",
        "                        'GCM5_ppt', 'GCM5_tass_max', 'GCM5_tass_min',\n",
        "                        'GCM6_ppt', 'GCM6_tass_max', 'GCM6_tass_min',\n",
        "                        'GCM7_ppt', 'GCM7_tass_max', 'GCM7_tass_min',\n",
        "                        'GCM8_ppt', 'GCM8_tass_max', 'GCM8_tass_min',\n",
        "                        'GCM9_ppt', 'GCM9_tass_max', 'GCM9_tass_min',\n",
        "                        'GCM10_ppt', 'GCM10_tass_max', 'GCM10_tass_min'\n",
        "                        ],\n",
        "                       gcm_data_frames_list):\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Loop through each DataFrame and check for null values after filling\n",
        "data_frames_with_nulls = []\n",
        "for df_name, df in zip(['Observed_ppt', 'Observed_tass_max', 'Observed_tass_min',\n",
        "                        'GCM1_ppt', 'GCM1_tass_max', 'GCM1_tass_min',\n",
        "                        'GCM2_ppt', 'GCM2_tass_max', 'GCM2_tass_min',\n",
        "                        'GCM3_ppt', 'GCM3_tass_max', 'GCM3_tass_min',\n",
        "                        'GCM4_ppt', 'GCM4_tass_max', 'GCM4_tass_min',\n",
        "                        'GCM5_ppt', 'GCM5_tass_max', 'GCM5_tass_min',\n",
        "                        'GCM6_ppt', 'GCM6_tass_max', 'GCM6_tass_min',\n",
        "                        'GCM7_ppt', 'GCM7_tass_max', 'GCM7_tass_min',\n",
        "                        'GCM8_ppt', 'GCM8_tass_max', 'GCM8_tass_min',\n",
        "                        'GCM9_ppt', 'GCM9_tass_max', 'GCM9_tass_min',\n",
        "                        'GCM10_ppt', 'GCM10_tass_max', 'GCM10_tass_min'\n",
        "                        ],\n",
        "                       gcm_data_frames_list):\n",
        "    if df.isnull().values.any():\n",
        "        data_frames_with_nulls.append(df_name)\n",
        "\n",
        "# Display DataFrame names with null values after filling\n",
        "print(\"DataFrames with null values after filling:\")\n",
        "for df_name in data_frames_with_nulls:\n",
        "    print(df_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of all GCM DataFrames\n",
        "gcm_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min,\n",
        "                        GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min\n",
        "                        ]\n",
        "\n",
        "# Loop through each DataFrame and check for null values\n",
        "for df_name, df in zip(['Observed_ppt', 'Observed_tass_max', 'Observed_tass_min',\n",
        "                        'GCM1_ppt', 'GCM1_tass_max', 'GCM1_tass_min',\n",
        "                        'GCM2_ppt', 'GCM2_tass_max', 'GCM2_tass_min',\n",
        "                        'GCM3_ppt', 'GCM3_tass_max', 'GCM3_tass_min',\n",
        "                        'GCM4_ppt', 'GCM4_tass_max', 'GCM4_tass_min',\n",
        "                        'GCM5_ppt', 'GCM5_tass_max', 'GCM5_tass_min',\n",
        "                        'GCM6_ppt', 'GCM6_tass_max', 'GCM6_tass_min',\n",
        "                        'GCM7_ppt', 'GCM7_tass_max', 'GCM7_tass_min',\n",
        "                        'GCM8_ppt', 'GCM8_tass_max', 'GCM8_tass_min',\n",
        "                        'GCM9_ppt', 'GCM9_tass_max', 'GCM9_tass_min',\n",
        "                        'GCM10_ppt', 'GCM10_tass_max', 'GCM10_tass_min'\n",
        "                        ],\n",
        "                       gcm_data_frames_list):\n",
        "    if df.isnull().values.any():\n",
        "        print(f\"DataFrame {df_name} contains null values.\")\n"
      ],
      "metadata": {
        "id": "BtlhG5i1bCwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4npOhYgybLB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "lYUbd4Ecb_t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcm_data_frames_list = [GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min,\n",
        "                        ]\n",
        "\n",
        "observed_data_frames_list = [Observed_ppt, Observed_tass_max, Observed_tass_min]\n"
      ],
      "metadata": {
        "id": "DFaWYMA9bK81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MiK7T0w1bK4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def calculate_nse(y_true, y_pred):\n",
        "    nse = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "    return nse\n",
        "\n",
        "def calculate_pbias(y_true, y_pred):\n",
        "    pbias = np.sum(y_pred - y_true) / np.sum(y_true) * 100\n",
        "    return pbias\n",
        "\n",
        "gcm_data_list = [GCM1_ppt, GCM1_tass_max, GCM1_tass_min,\n",
        "                        GCM2_ppt, GCM2_tass_max, GCM2_tass_min,\n",
        "                        GCM3_ppt, GCM3_tass_max, GCM3_tass_min,\n",
        "                        GCM4_ppt, GCM4_tass_max, GCM4_tass_min,\n",
        "                        GCM5_ppt, GCM5_tass_max, GCM5_tass_min,\n",
        "                        GCM6_ppt, GCM6_tass_max, GCM6_tass_min,\n",
        "                        GCM7_ppt, GCM7_tass_max, GCM7_tass_min,\n",
        "                        GCM8_ppt, GCM8_tass_max, GCM8_tass_min,\n",
        "                        GCM9_ppt, GCM9_tass_max, GCM9_tass_min,\n",
        "                        GCM10_ppt, GCM10_tass_max, GCM10_tass_min,\n",
        "]\n",
        "\n",
        "# Observed dataframes\n",
        "observed_data_list = [Observed_ppt, Observed_tass_max, Observed_tass_min]\n",
        "\n",
        "# Lists to store results\n",
        "r2_results = []\n",
        "nse_results = []\n",
        "pbias_results = []\n",
        "\n",
        "# Create directories for saving files\n",
        "if not os.path.exists('model_files'):\n",
        "    os.makedirs('model_files')\n",
        "\n",
        "if not os.path.exists('testing_predictions'):\n",
        "    os.makedirs('testing_predictions')\n",
        "\n",
        "if not os.path.exists('actual_testing_data'):\n",
        "    os.makedirs('actual_testing_data')\n",
        "\n",
        "if not os.path.exists('actual_testing_data_Xs'):\n",
        "    os.makedirs('actual_testing_data_Xs')\n",
        "\n",
        "\n",
        "# Loop through each GCM\n",
        "for i in range(1, 11):\n",
        "    gcm_start_idx = (i-1)*3\n",
        "    gcm_df_list = gcm_data_list[gcm_start_idx:gcm_start_idx+3]\n",
        "    # Concatenate GCM data\n",
        "    gcm_df = pd.concat(gcm_df_list, axis=1)\n",
        "\n",
        "    # Concatenate observed data\n",
        "    observed_df = pd.concat(observed_data_list, axis=1)\n",
        "\n",
        "    print(f\"Training model for GCM{i}\")\n",
        "\n",
        "    # Preprocess data\n",
        "    gcm_df = gcm_df.applymap(lambda x: 2.3 if x=='R' else (4.2 if x=='----' else x))\n",
        "    gcm_df = gcm_df.astype(float)\n",
        "\n",
        "    observed_df = observed_df.applymap(lambda x: 2.3 if x=='R' else (4.2 if x=='----' else x))\n",
        "    observed_df = observed_df.astype(float)\n",
        "\n",
        "    # Split data\n",
        "    X = gcm_df.values\n",
        "    y = observed_df.values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "    # Build a CNN model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 1), activation='relu', input_shape=(X_train.shape[1], 1, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 1)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(50, activation='relu'),\n",
        "        tf.keras.layers.Dense(50, activation='relu'),\n",
        "        tf.keras.layers.Dense(y_train.shape[1])  # Replace 'output_shape' with the number of output units you need\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train_reshaped, y_train, epochs=30, batch_size=256, verbose=0)\n",
        "\n",
        "    # Evaluate model\n",
        "    y_pred = model.predict(X_test_reshaped)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    nse = calculate_nse(y_test, y_pred)\n",
        "    pbias = calculate_pbias(y_test, y_pred)\n",
        "\n",
        "    print(f\"GCM {i} Results:\")\n",
        "    print(f\"R-squared: {r2}\")\n",
        "    print(f\"NSE: {nse}\")\n",
        "    print(f\"PBIAS: {pbias}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Save model\n",
        "    model.save(os.path.join('model_files', f'gcm_model_{i}.keras'))\n",
        "\n",
        "    # Save results\n",
        "    r2_results.append(r2)\n",
        "    nse_results.append(nse)\n",
        "    pbias_results.append(pbias)\n",
        "\n",
        "    # Save testing predictions and actual testing data to CSV\n",
        "    testing_predictions_df = pd.DataFrame(data=y_pred, columns=observed_df.columns)\n",
        "    actual_testing_data_df = pd.DataFrame(data=y_test, columns=observed_df.columns)\n",
        "    actual_testing_data_Xs = pd.DataFrame(data=X_test, columns=observed_df.columns)\n",
        "\n",
        "    testing_predictions_df.to_csv(os.path.join('testing_predictions', f'testing_predictions_gcm{i}.csv'), index=False)\n",
        "    actual_testing_data_df.to_csv(os.path.join('actual_testing_data', f'actual_testing_data_gcm{i}.csv'), index=False)\n",
        "    actual_testing_data_Xs.to_csv(os.path.join('actual_testing_data_Xs', f'actual_testing_data_Xs_gcm{i}.csv'), index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CJNr4PX4pOe2",
        "outputId": "65700bc5-5897-4af6-9635-4184b92e9101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for GCM1\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 1 Results:\n",
            "R-squared: 0.5179675851206341\n",
            "NSE: 0.8330045003283544\n",
            "PBIAS: 6.814550745562481\n",
            "\n",
            "\n",
            "Training model for GCM2\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 2 Results:\n",
            "R-squared: 0.5360275533174914\n",
            "NSE: 0.835263574658264\n",
            "PBIAS: -1.086742954226385\n",
            "\n",
            "\n",
            "Training model for GCM3\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 3 Results:\n",
            "R-squared: 0.5259611693880726\n",
            "NSE: 0.8299043753068546\n",
            "PBIAS: 2.6353995583879226\n",
            "\n",
            "\n",
            "Training model for GCM4\n",
            "104/104 [==============================] - 0s 3ms/step\n",
            "GCM 4 Results:\n",
            "R-squared: 0.5427132782938009\n",
            "NSE: 0.8383186191594353\n",
            "PBIAS: 0.7453750079902015\n",
            "\n",
            "\n",
            "Training model for GCM5\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 5 Results:\n",
            "R-squared: 0.48120584781814596\n",
            "NSE: 0.8070496640733924\n",
            "PBIAS: -7.842082418747011\n",
            "\n",
            "\n",
            "Training model for GCM6\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 6 Results:\n",
            "R-squared: 0.43360077546118103\n",
            "NSE: 0.7789369954680889\n",
            "PBIAS: 5.707573836456544\n",
            "\n",
            "\n",
            "Training model for GCM7\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 7 Results:\n",
            "R-squared: 0.4827591322202072\n",
            "NSE: 0.8060505110264539\n",
            "PBIAS: -2.619519613345687\n",
            "\n",
            "\n",
            "Training model for GCM8\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 8 Results:\n",
            "R-squared: 0.5252094105209514\n",
            "NSE: 0.8292894417776879\n",
            "PBIAS: 0.4276300303213593\n",
            "\n",
            "\n",
            "Training model for GCM9\n",
            "104/104 [==============================] - 0s 2ms/step\n",
            "GCM 9 Results:\n",
            "R-squared: 0.5209618495229298\n",
            "NSE: 0.8274254795877254\n",
            "PBIAS: -1.134124666580072\n",
            "\n",
            "\n",
            "Training model for GCM10\n",
            "104/104 [==============================] - 0s 3ms/step\n",
            "GCM 10 Results:\n",
            "R-squared: 0.3238502908951111\n",
            "NSE: 0.7225068087464657\n",
            "PBIAS: 4.6341580319817695\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-fa8d71a6fdd6>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;31m# Create a summary dataframe for results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m results_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;34m'GCM'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;34m'R-squared'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr2_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summary dataframe for results\n",
        "results_df = pd.DataFrame({\n",
        "    'GCM': range(1, 11),\n",
        "    'R-squared': r2_results,\n",
        "    'NSE': nse_results,\n",
        "    'PBIAS': pbias_results\n",
        "})\n",
        "\n",
        "# Save results summary to CSV\n",
        "results_df.to_csv('results_summary.csv', index=False)"
      ],
      "metadata": {
        "id": "D-MFcV0HaDbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/testing_predictions'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'testing_predictions.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "id": "w--q2xJvqLkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd01c79b-c964-49ff-afbf-49d3843ace29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'testing_predictions.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/actual_testing_data_Xs'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'actual_testing_data_Xs.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDGmjQRFqLgf",
        "outputId": "442065af-cc7e-48b6-ab27-1bdc91d1e3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'actual_testing_data_Xs.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/actual_testing_data'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'actual_testing_data.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNRlXUMP67wa",
        "outputId": "3e844318-6d78-4e77-dd32-03b02376179f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'actual_testing_data.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5GZ6r3d62M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/model_files'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'model_files.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLY9oq4_rNIr",
        "outputId": "232bb8f6-7c5d-4526-e5ff-5a7d1f402ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'model_files.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the folder you want to zip\n",
        "folder_to_zip = '/content/CNN_results_1-10'  # Change to the path of your folder\n",
        "\n",
        "# Specify the name of the zip file you want to create\n",
        "zip_file_name = 'CNN_results_1-10.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Walk through the folder and add each file to the zip\n",
        "    for root, _, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Zip file '{zip_file_name}' created.\")\n"
      ],
      "metadata": {
        "id": "GPnuDuSWrSOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163285a7-12e5-45ee-e9a3-5917ba332abf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zip file 'CNN_results_1-10.zip' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8o1lWKtakAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}